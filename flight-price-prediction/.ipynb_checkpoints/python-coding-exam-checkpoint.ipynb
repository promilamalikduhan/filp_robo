{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4445e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03eb56ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/promilamalik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7076e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/promilamalik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5451866",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='my name is promila malik'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "073078ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'promila', 'malik']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a499c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"I m a good citizen of india but i dont  wanna to be a part of india anymore,want to fly,seen the rest of the world.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc45d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I m a good citizen of india but i dont  wanna to be a part of india anymore,want to fly,seen the rest of the world.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc65ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m',\n",
       " 'a',\n",
       " 'good',\n",
       " 'citizen',\n",
       " 'of',\n",
       " 'india',\n",
       " 'but',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'wanna',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'part',\n",
       " 'of',\n",
       " 'india',\n",
       " 'anymore',\n",
       " 'want',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'seen',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(text1,\"[a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c53ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(text1,\"[A-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0063122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I m a good citizen of india but i dont  wanna to be a part of india anymore,want to fly,seen the rest of the world.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(text1,\"[\\a-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c469cb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m',\n",
       " 'a',\n",
       " 'good',\n",
       " 'citizen',\n",
       " 'of',\n",
       " 'india',\n",
       " 'but',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'wanna',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'part',\n",
       " 'of',\n",
       " 'india',\n",
       " 'anymore',\n",
       " 'want',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'seen',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(text1,\"[a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721c57fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '  ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ',',\n",
       " ' ',\n",
       " ' ',\n",
       " ',',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(text1,\"[^a-z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "647169f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(text1,\"[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5daef116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(text1,\"[$]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a15bd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I m a good citizen of india but i dont  wanna to be a part of india anymore,want to fly,seen the rest of the world.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(text1,\"[^0-9']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0464a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a90998e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "856719fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e3cae69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cdd2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02559ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9409d4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84c5bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset.update('new','old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d126106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7abd73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80dcdc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85cbf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=string.punctuation\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27c1e922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e2c1b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original lenght 24\n",
      "len of cleaned text 3\n",
      "\n",
      " ['name', 'promila', 'malik']\n"
     ]
    }
   ],
   "source": [
    "cleaned_text=[]\n",
    "\n",
    "\n",
    "for word in nltk.word_tokenize(text):\n",
    "    \n",
    "    if word not in punc:\n",
    "        if word not in stop_words:\n",
    "            cleaned_text.append(word)\n",
    "print('original lenght',len(text))\n",
    "print('len of cleaned text',len(cleaned_text))\n",
    "print('\\n',cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa893833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is promila malik\n"
     ]
    }
   ],
   "source": [
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69d396ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY NAME IS PROMILA MALIK\n"
     ]
    }
   ],
   "source": [
    "print(text.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170f26a",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b566199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,LancasterStemmer,SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcb37fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "lancaster stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'the',\n",
       " 'office',\n",
       " 'on',\n",
       " 'my',\n",
       " 'bike',\n",
       " 'when',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'car',\n",
       " 'passing',\n",
       " 'by',\n",
       " 'hit',\n",
       " 'the',\n",
       " 'tree',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster=LancasterStemmer()\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "snowball=SnowballStemmer('english')\n",
    "\n",
    "print('porter stemmer')\n",
    "print(porter.stem('hobby'))\n",
    "print(porter.stem('hobbies'))\n",
    "print(porter.stem('computer'))\n",
    "print(porter.stem('computation'))\n",
    "\n",
    "print('lancaster stemmer')\n",
    "print(porter.stem('hobby'))\n",
    "print(porter.stem('hobbies'))\n",
    "print(porter.stem('computer'))\n",
    "print(porter.stem('computation'))\n",
    "\n",
    "\n",
    "sentence= ' I was going to the office on my bike when i saw a car passing by hit the tree.'\n",
    "\n",
    "token=list(nltk.word_tokenize(sentence))\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2a78ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I was going to the office on my bike when i saw a car passing by hit the tree.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c1c4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was go to the offic on my bike when i saw a car pass by hit the tree .\n",
      "i was going to the off on my bik when i saw a car pass by hit the tre .\n",
      "i wa go to the offic on my bike when i saw a car pass by hit the tree .\n"
     ]
    }
   ],
   "source": [
    "for stemmer in(snowball,lancaster,porter):\n",
    "    stem=[stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2e7010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"running\"))\n",
    "print(porter.stem(\"runs\"))\n",
    "print(porter.stem(\"ran\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb187540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/promilamalik/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e431d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "379c3423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize(\"running\"))\n",
    "print(lemma.lemmatize(\"runs\"))\n",
    "print(lemma.lemmatize(\"ran\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adc70465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize(\"running\",pos='v'))\n",
    "print(lemma.lemmatize(\"runs\",pos='v'))\n",
    "print(lemma.lemmatize(\"ran\",pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a282d406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for Bring is bring \n",
      "Stemming for King is king \n",
      "Stemming for Going is go \n",
      "Stemming for Anything is anyth \n",
      "Stemming for Sing is sing \n",
      "Stemming for Ring is ring \n",
      "Stemming for Nothing is noth \n",
      "Stemming for Thing is thing \n"
     ]
    }
   ],
   "source": [
    "text='Bring King Going Anything Sing Ring Nothing Thing'\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n",
    "\n",
    "tokenization=nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {} \".format(w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7153356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Bring is Bring\n",
      "Lemma for King is King\n",
      "Lemma for Going is Going\n",
      "Lemma for Anything is Anything\n",
      "Lemma for Sing is Sing\n",
      "Lemma for Ring is Ring\n",
      "Lemma for Nothing is Nothing\n",
      "Lemma for Thing is Thing\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "tokenization=nltk.word_tokenize(text)\n",
    "\n",
    "for w in tokenization:\n",
    "    print('Lemma for {} is {}'.format(w,wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1004d470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms = {'alive', 'active_voice', 'fighting', 'active', 'combat-ready', 'participating', 'dynamic', 'active_agent'}\n",
      "synonyms = {'stative', 'dormant', 'passive', 'quiet', 'passive_voice', 'extinct', 'inactive'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets('active'):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "            \n",
    "print(\"synonyms =\",set(synonyms))  \n",
    "print(\"synonyms =\",set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e380f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms = {'huffy', 'unbalanced', 'brainsick', 'excited', 'demented', 'sick', 'sore', 'unrestrained', 'harebrained', 'insane', 'mad', 'unhinged', 'frantic', 'crazy', 'disturbed', 'delirious'}\n",
      "synonyms = set()\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets('mad'):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "            \n",
    "print(\"synonyms =\",set(synonyms))  \n",
    "print(\"synonyms =\",set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0437811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['bag', 'example', 'is', 'of', 'the', 'this', 'words']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/promilamalik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "string=['this is the example of bag of words']\n",
    "\n",
    "vect1=CountVectorizer()\n",
    "vect1.fit_transform(string)\n",
    "print('bag of words :',vect1.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "535581c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 2, 'the': 4, 'example': 1, 'of': 3, 'bag': 0, 'words': 6}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac7b38b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vect=CountVectorizer()\n",
    "\n",
    "c_vect.fit(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86b7c593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text present at [[0 0 2 1 0 0 1]]\n",
      "original indexes ['bag', 'example', 'is', 'of', 'the', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "string2=['lets understand is of words is']\n",
    "\n",
    "c_new_vect=c_vect.transform(string2)\n",
    "\n",
    "print('text present at',c_new_vect.toarray())\n",
    "\n",
    "print('original indexes',vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9fc621e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...])\n"
     ]
    }
   ],
   "source": [
    "stpwords=stopwords.words('english')\n",
    "\n",
    "string=['this is an example of bag of words']\n",
    "vect1=CountVectorizer(stop_words=stpwords)\n",
    "print(vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d0d9698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['bag', 'example', 'words']\n",
      "vocab : {'example': 1, 'bag': 0, 'words': 2}\n"
     ]
    }
   ],
   "source": [
    "vect1.fit_transform(string)\n",
    "print('bag of words :',vect1.get_feature_names())\n",
    "print('vocab :',vect1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6467e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_matrix(message,countvect):\n",
    "    terms_doc=countvect.fit_transform(message)\n",
    "    return pd.DataFrame(terms_doc.toarray(),columns=countvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "255edfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "below metrix is the bag of words approach\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>but</th>\n",
       "      <th>can</th>\n",
       "      <th>cant</th>\n",
       "      <th>course</th>\n",
       "      <th>data</th>\n",
       "      <th>get</th>\n",
       "      <th>have</th>\n",
       "      <th>in</th>\n",
       "      <th>job</th>\n",
       "      <th>making</th>\n",
       "      <th>my</th>\n",
       "      <th>not</th>\n",
       "      <th>or</th>\n",
       "      <th>progress</th>\n",
       "      <th>science</th>\n",
       "      <th>slowly</th>\n",
       "      <th>understand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   but  can  cant  course  data  get  have  in  job  making  my  not  or  \\\n",
       "0    0    0     0       1     1    0     1   1    0       1   1    0   0   \n",
       "1    1    1     1       0     0    1     0   0    1       0   0    1   1   \n",
       "\n",
       "   progress  science  slowly  understand  \n",
       "0         1        1       2           0  \n",
       "1         0        0       0           1  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message=['i have slowly slowly making progress in my data science course', \n",
    "         'but i cant understand can i get a job or not']\n",
    "\n",
    "c_vect=CountVectorizer()\n",
    "print('below metrix is the bag of words approach')\n",
    "text_matrix(message,c_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68a693a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram : ['an', 'example', 'gram', 'is', 'of', 'this']\n",
      "2-gram : ['an', 'example', 'gram', 'is', 'of', 'this']\n",
      "3-gram : ['an', 'example', 'gram', 'is', 'of', 'this']\n",
      "4-gram : ['an', 'example', 'gram', 'is', 'of', 'this']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/promilamalik/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string=['this is an example of gram']\n",
    "\n",
    "vect1=CountVectorizer(ngram_range=(1,1))\n",
    "vect1.fit_transform(string)\n",
    "vect2=CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "vect2.fit_transform(string)\n",
    "vect3=CountVectorizer(ngram_range=(3,3))\n",
    "vect3.fit_transform(string)\n",
    "\n",
    "vect4=CountVectorizer(ngram_range=(4,4))\n",
    "\n",
    "vect4.fit_transform(string)\n",
    "\n",
    "print('1-gram :',vect1.get_feature_names())\n",
    "\n",
    "print('2-gram :',vect1.get_feature_names())\n",
    "print('3-gram :',vect1.get_feature_names())\n",
    "print('4-gram :',vect1.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726ba54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
